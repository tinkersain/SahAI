"""
SahAI AI Service - LLM Integration for Hindi Conversations
Gemini handles ALL processing - understanding, eligibility, responses
Supports the agentic workflow with tool orchestration
"""
import json
from typing import Dict, Any, Optional, List

try:
    from config.settings import settings
except ImportError:
    class MockSettings:
        class AI:
            gemini_api_key = ""
            gemini_model = "gemini-2.0-flash"
        ai = AI()
    settings = MockSettings()


class AIService:
    """
    AI Service - Gemini handles everything
    All scheme logic, eligibility checks, and responses are done by LLM
    Supports agentic workflow with planning and response generation
    """
    
    def __init__(self):
        self.client = None
        self.model_name = None
        self._init_gemini()
    
    def _init_gemini(self):
        """Initialize Gemini model"""
        api_key = getattr(settings.ai, 'gemini_api_key', '') or ""
        
        if not api_key:
            print("тЪая╕П Gemini API key not set - AI features limited")
            return
        
        try:
            from google import genai
            self.client = genai.Client(api_key=api_key)
            self.model_name = getattr(settings.ai, 'gemini_model', 'gemini-2.0-flash')
            print(f"тЬЕ Gemini initialized: {self.model_name}")
        except ImportError:
            print("тЪая╕П google-genai not installed")
        except Exception as e:
            print(f"тЪая╕П Gemini init failed: {e}")
    
    def generate_response(self, prompt: str, temperature: float = 0.7) -> str:
        """
        Generate a response from the LLM
        
        Args:
            prompt: The complete prompt with all context
            temperature: Creativity level (0-1)
            
        Returns:
            Generated response text
        """
        if not self.client:
            return self._fallback_response("")
        
        try:
            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=temperature,
                    max_output_tokens=2000
                )
            )
            
            return response.text.strip()
            
        except Exception as e:
            print(f"LLM generation error: {e}")
            return self._fallback_response("")
    
    def analyze_intent(self, user_input: str, conversation_history: str = "") -> Dict[str, Any]:
        """
        Analyze user intent using LLM
        
        Returns:
            Dict with intent, confidence, entities
        """
        if not self.client:
            return {"intent": "unknown", "confidence": 0.5, "entities": {}}
        
        prompt = f"""рдЖрдк рдПрдХ intent classifier рд╣реИрдВред рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХреЗ рд╕рдВрджреЗрд╢ рдХрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХрд░реЗрдВред

рд╕рдВрджреЗрд╢: "{user_input}"

рдкрд┐рдЫрд▓реА рдмрд╛рддрдЪреАрдд:
{conversation_history}

JSON рдореЗрдВ рдЬрд╡рд╛рдм рджреЗрдВ:
{{
    "intent": "greeting|farewell|eligibility_check|scheme_inquiry|application_help|document_info|provide_info|correction|general",
    "confidence": 0.0-1.0,
    "entities": {{
        "scheme_mentioned": "scheme_id or null",
        "age_mentioned": number or null,
        "income_mentioned": number or null,
        "category_mentioned": "SC|ST|OBC|General or null"
    }},
    "requires_tools": ["tool1", "tool2"]
}}

рдХреЗрд╡рд▓ JSON рджреЗрдВ, рдХреБрдЫ рдФрд░ рдирд╣реАрдВ:"""

        try:
            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                    max_output_tokens=500
                )
            )
            
            # Parse JSON response
            text = response.text.strip()
            # Clean up markdown if present
            if text.startswith("```"):
                text = text.split("```")[1]
                if text.startswith("json"):
                    text = text[4:]
            
            return json.loads(text)
            
        except Exception as e:
            print(f"Intent analysis error: {e}")
            return {"intent": "general", "confidence": 0.5, "entities": {}}
    
    def generate_tool_selection(self, intent: str, user_input: str, 
                                available_tools: List[str]) -> List[str]:
        """
        Use LLM to select appropriate tools for a task
        
        Returns:
            List of tool names to execute
        """
        if not self.client:
            # Fallback logic
            if "рдкрд╛рддреНрд░" in user_input or "eligible" in user_input.lower():
                return ["eligibility_engine", "scheme_retrieval"]
            return ["scheme_retrieval"]
        
        prompt = f"""рдЖрдк рдПрдХ tool selector рд╣реИрдВред рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХреЗ intent рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рд╕рд╣реА tools рдЪреБрдиреЗрдВред

Intent: {intent}
User query: "{user_input}"

Available tools:
{json.dumps(available_tools)}

Tool descriptions:
- eligibility_engine: Check if user is eligible for schemes based on their data
- scheme_retrieval: Get information about schemes
- document_checker: List required documents for a scheme
- application_status: Check status of an application
- user_data_extractor: Extract user info from text

Return JSON array of tool names to use (in order):"""

        try:
            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    max_output_tokens=200
                )
            )
            
            text = response.text.strip()
            if text.startswith("```"):
                text = text.split("```")[1]
                if text.startswith("json"):
                    text = text[4:]
            
            tools = json.loads(text)
            return [t for t in tools if t in available_tools]
            
        except Exception as e:
            print(f"Tool selection error: {e}")
            return ["scheme_retrieval"]
    
    def process_query(
        self,
        user_input: str,
        schemes_context: str,
        conversation_history: str,
        user_data: Dict[str, Any]
    ) -> str:
        """
        Main method - Gemini processes everything (backward compatible)
        
        Args:
            user_input: User's current query in Hindi
            schemes_context: All scheme information as context
            conversation_history: Previous conversation turns
            user_data: Extracted user information (age, income, etc.)
            
        Returns:
            Complete Hindi response from Gemini
        """
        if not self.client:
            return self._fallback_response(user_input)
        
        try:
            system_prompt = self._build_system_prompt(schemes_context)
            user_context = self._build_user_context(user_data, conversation_history)
            
            full_prompt = f"""{system_prompt}

{user_context}

рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХрд╛ рд╡рд░реНрддрдорд╛рди рд╕рдВрджреЗрд╢: "{user_input}"

рдХреГрдкрдпрд╛ рдЙрдкрдпреБрдХреНрдд рд╣рд┐рдВрджреА рдореЗрдВ рдЬрд╡рд╛рдм рджреЗрдВ:"""

            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    max_output_tokens=1000
                )
            )
            
            return response.text.strip()
            
        except Exception as e:
            print(f"AI process error: {e}")
            return self._fallback_response(user_input)
    
    def evaluate_response_quality(self, response: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate quality of a generated response
        
        Returns:
            Dict with quality_score, issues, suggestions
        """
        if not self.client:
            return {"quality_score": 0.5, "issues": [], "suggestions": []}
        
        prompt = f"""Evaluate this Hindi response for quality:

Response: "{response}"

Context: {json.dumps(context, ensure_ascii=False)}

Rate on:
1. Relevance (0-1): Does it answer the query?
2. Completeness (0-1): Is the information complete?
3. Clarity (0-1): Is it clear and easy to understand?
4. Helpfulness (0-1): Does it help the user?

Return JSON:
{{
    "quality_score": 0.0-1.0,
    "issues": ["issue1", "issue2"],
    "suggestions": ["suggestion1"]
}}"""

        try:
            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    max_output_tokens=300
                )
            )
            
            text = response.text.strip()
            if text.startswith("```"):
                text = text.split("```")[1]
                if text.startswith("json"):
                    text = text[4:]
            
            return json.loads(text)
            
        except Exception as e:
            print(f"Quality evaluation error: {e}")
            return {"quality_score": 0.7, "issues": [], "suggestions": []}
    
    def _build_system_prompt(self, schemes_context: str) -> str:
        """Build the system prompt with all scheme information"""
        return f"""рдЖрдк "рд╕рд╣рд╛рдИ" рд╣реИрдВ - рдПрдХ рдорд╣рд┐рд▓рд╛ рд╣рд┐рдВрджреА рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛ рд╕рд╣рд╛рдпрд┐рдХрд╛ред рдЖрдк рдПрдХ рдорджрджрдЧрд╛рд░ рдмрд╣рди рдХреА рддрд░рд╣ рдмрд╛рдд рдХрд░рддреА рд╣реИрдВред

ЁЯОп рдЖрдкрдХрд╛ рдХрд╛рдо:
1. рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛рдУрдВ рдХреА рдЬрд╛рдирдХрд╛рд░реА рджреЗрдирд╛
2. рдкрд╛рддреНрд░рддрд╛ рдЬрд╛рдВрдЪрдирд╛
3. рдЖрд╡реЗрджрди рдкреНрд░рдХреНрд░рд┐рдпрд╛ рд╕рдордЭрд╛рдирд╛

ЁЯУЛ рдпреЛрдЬрдирд╛рдПрдВ:
{schemes_context}

тЪая╕П рдмрд╣реБрдд рдорд╣рддреНрд╡рдкреВрд░реНрдг - Voice Output рдХреЗ рд▓рд┐рдП:
- рдЬрд╡рд╛рдм рдмрд╣реБрдд рдЫреЛрдЯрд╛ рд░рдЦреЗрдВ (рдЕрдзрд┐рдХрддрдо 3-4 рд╡рд╛рдХреНрдп)
- рд╕рд┐рд░реНрдл рд╕рдмрд╕реЗ рдЬрд░реВрд░реА рдмрд╛рдд рдмрддрд╛рдПрдВ
- рд▓рдВрдмреА рд╕реВрдЪреА рди рджреЗрдВ, рд╕рд┐рд░реНрдл 1-2 рдпреЛрдЬрдирд╛ рдмрддрд╛рдПрдВ
- рдЗрдореЛрдЬреА рдХрдо рд╕реЗ рдХрдо рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ
- рдпрджрд┐ рдЬрд╛рдирдХрд╛рд░реА рдЪрд╛рд╣рд┐рдП рддреЛ рд╕реАрдзреЗ рдкреВрдЫреЗрдВ
- рдорд╣рд┐рд▓рд╛ рдХреЗ рд░реВрдк рдореЗрдВ рдмрд╛рдд рдХрд░реЗрдВ (рдЬреИрд╕реЗ: "рдореИрдВ рдмрддрд╛рддреА рд╣реВрдВ", "рдореИрдВрдиреЗ рд╕рдордЭрд╛", "рдореИрдВ рдЖрдкрдХреА рдорджрдж рдХрд░реВрдВрдЧреА")

ЁЯУМ рдирд┐рдпрдо:
- рдХреЗрд╡рд▓ рд╣рд┐рдВрджреА рдореЗрдВ рдЬрд╡рд╛рдм рджреЗрдВ
- рдмрд╣реБрдд рд╕рдВрдХреНрд╖рд┐рдкреНрдд рд░рд╣реЗрдВ
- рдпрджрд┐ рдкрд╛рддреНрд░рддрд╛ рдЬрд╛рдВрдЪрдиреА рд╣реИ рддреЛ рдкрд╣рд▓реЗ рдЙрдореНрд░ рдФрд░ рдЖрдп рдкреВрдЫреЗрдВ
- рд╣рдореЗрд╢рд╛ рдорд╣рд┐рд▓рд╛ рдХреА рднрд╛рд╖рд╛ рд╢реИрд▓реА рдореЗрдВ рдмреЛрд▓реЗрдВ (feminine verbs)"""

    def _build_user_context(self, user_data: Dict[str, Any], conversation_history: str) -> str:
        """Build context about the user"""
        context_parts = []
        
        if user_data:
            context_parts.append(f"рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХреА рдЬрд╛рдирдХрд╛рд░реА: {json.dumps(user_data, ensure_ascii=False)}")
        
        if conversation_history:
            context_parts.append(f"рдкрд┐рдЫрд▓реА рдмрд╛рддрдЪреАрдд:\n{conversation_history}")
        
        if context_parts:
            return "ЁЯУЭ рд╕рдВрджрд░реНрдн:\n" + "\n".join(context_parts)
        return ""
    
    def get_greeting(self) -> str:
        """Get initial greeting"""
        if not self.client:
            return self._default_greeting()
        
        try:
            prompt = """рдЖрдк "рд╕рд╣рд╛рдИ" рд╣реИрдВ - рдПрдХ рдорд╣рд┐рд▓рд╛ рд╣рд┐рдВрджреА рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛ рд╕рд╣рд╛рдпрд┐рдХрд╛ред рдЖрдк рдПрдХ рдорджрджрдЧрд╛рд░ рдмрд╣рди рдХреА рддрд░рд╣ рдмрд╛рдд рдХрд░рддреА рд╣реИрдВред
            
рдПрдХ рдорд┐рддреНрд░рд╡рдд рд╕реНрд╡рд╛рдЧрдд рд╕рдВрджреЗрд╢ рджреЗрдВ рдЬреЛ рдмрддрд╛рдП рдХрд┐ рдЖрдк рдХреНрдпрд╛-рдХреНрдпрд╛ рдХрд░ рд╕рдХрддреА рд╣реИрдВред
рдорд╣рд┐рд▓рд╛ рдХреЗ рд░реВрдк рдореЗрдВ рдмрд╛рдд рдХрд░реЗрдВ (рдЬреИрд╕реЗ: "рдореИрдВ рдмрддрд╛рддреА рд╣реВрдВ", "рдореИрдВ рдЖрдкрдХреА рдорджрдж рдХрд░реВрдВрдЧреА")ред
рд╕рдВрдХреНрд╖рд┐рдкреНрдд рд░рдЦреЗрдВ (4-5 рд╡рд╛рдХреНрдп)ред рдЗрдореЛрдЬреА рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВред"""

            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.8,
                    max_output_tokens=300
                )
            )
            
            return response.text.strip()
            
        except Exception as e:
            print(f"Greeting error: {e}")
            return self._default_greeting()
    
    def _default_greeting(self) -> str:
        """Default greeting when AI is unavailable"""
        return """рдирдорд╕реНрддреЗ! ЁЯЩП рдореИрдВ рд╕рд╣рд╛рдИ рд╣реВрдВ, рдЖрдкрдХреА рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛ рд╕рд╣рд╛рдпрд┐рдХрд╛ред

рдореИрдВ рдЖрдкрдХреЛ рдЗрди рдЪреАрдЬрд╝реЛрдВ рдореЗрдВ рдорджрдж рдХрд░ рд╕рдХрддреА рд╣реВрдВ:
тАв рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛рдУрдВ рдХреА рдЬрд╛рдирдХрд╛рд░реА
тАв рдкрд╛рддреНрд░рддрд╛ рдЬрд╛рдВрдЪ
тАв рдЖрд╡реЗрджрди рдкреНрд░рдХреНрд░рд┐рдпрд╛

рдмрддрд╛рдЗрдП, рдЖрдкрдХреЛ рдХрд┐рд╕ рдпреЛрдЬрдирд╛ рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдЬрд╛рдирдирд╛ рд╣реИ?"""

    def _fallback_response(self, user_input: str) -> str:
        """Fallback response when AI is unavailable"""
        return """рдХреНрд╖рдорд╛ рдХрд░реЗрдВ, рдореИрдВ рдЕрднреА рдЖрдкрдХреА рдорджрдж рдХрд░рдиреЗ рдореЗрдВ рдЕрд╕рдорд░реНрде рд╣реВрдВред

рдХреГрдкрдпрд╛ рдереЛрдбрд╝реА рджреЗрд░ рдмрд╛рдж рдлрд┐рд░ рд╕реЗ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВ рдпрд╛ рд╣реЗрд▓реНрдкрд▓рд╛рдЗрди рдкрд░ рдХреЙрд▓ рдХрд░реЗрдВ: 1800-111-555"""
