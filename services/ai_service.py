"""
SahAI AI Service - LLM Integration for Hindi Conversations
Gemini handles ALL processing - understanding, eligibility, responses
"""
import json
from typing import Dict, Any, Optional

try:
    from config.settings import settings
except ImportError:
    class MockSettings:
        class AI:
            gemini_api_key = ""
            gemini_model = "gemini-2.0-flash"
        ai = AI()
    settings = MockSettings()


class AIService:
    """
    AI Service - Gemini handles everything
    All scheme logic, eligibility checks, and responses are done by LLM
    """
    
    def __init__(self):
        self.client = None
        self.model_name = None
        self._init_gemini()
    
    def _init_gemini(self):
        """Initialize Gemini model"""
        api_key = getattr(settings.ai, 'gemini_api_key', '') or ""
        
        if not api_key:
            print("тЪая╕П Gemini API key not set - AI features limited")
            return
        
        try:
            from google import genai
            self.client = genai.Client(api_key=api_key)
            self.model_name = getattr(settings.ai, 'gemini_model', 'gemini-2.0-flash')
            print(f"тЬЕ Gemini initialized: {self.model_name}")
        except ImportError:
            print("тЪая╕П google-genai not installed")
        except Exception as e:
            print(f"тЪая╕П Gemini init failed: {e}")
    
    def process_query(
        self,
        user_input: str,
        schemes_context: str,
        conversation_history: str,
        user_data: Dict[str, Any]
    ) -> str:
        """
        Main method - Gemini processes everything
        
        Args:
            user_input: User's current query in Hindi
            schemes_context: All scheme information as context
            conversation_history: Previous conversation turns
            user_data: Extracted user information (age, income, etc.)
            
        Returns:
            Complete Hindi response from Gemini
        """
        if not self.client:
            return self._fallback_response(user_input)
        
        try:
            system_prompt = self._build_system_prompt(schemes_context)
            user_context = self._build_user_context(user_data, conversation_history)
            
            full_prompt = f"""{system_prompt}

{user_context}

рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХрд╛ рд╡рд░реНрддрдорд╛рди рд╕рдВрджреЗрд╢: "{user_input}"

рдХреГрдкрдпрд╛ рдЙрдкрдпреБрдХреНрдд рд╣рд┐рдВрджреА рдореЗрдВ рдЬрд╡рд╛рдм рджреЗрдВ:"""

            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    max_output_tokens=1000
                )
            )
            
            return response.text.strip()
            
        except Exception as e:
            print(f"AI process error: {e}")
            return self._fallback_response(user_input)
    
    def _build_system_prompt(self, schemes_context: str) -> str:
        """Build the system prompt with all scheme information"""
        return f"""рдЖрдк "рд╕рд╣рд╛рдИ" рд╣реИрдВ - рдПрдХ рдмреБрджреНрдзрд┐рдорд╛рди рд╣рд┐рдВрджреА рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛ рд╕рд╣рд╛рдпрдХред рдЖрдк рднрд╛рд░рдд рд╕рд░рдХрд╛рд░ рдХреА рдХрд▓реНрдпрд╛рдгрдХрд╛рд░реА рдпреЛрдЬрдирд╛рдУрдВ рдореЗрдВ рд▓реЛрдЧреЛрдВ рдХреА рдорджрдж рдХрд░рддреЗ рд╣реИрдВред

ЁЯОп рдЖрдкрдХрд╛ рдХрд╛рдо:
1. рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛рдУрдВ рдХреА рдЬрд╛рдирдХрд╛рд░реА рджреЗрдирд╛
2. рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХреА рдкрд╛рддреНрд░рддрд╛ рдЬрд╛рдВрдЪрдирд╛ (рдЙрдореНрд░, рдЖрдп, рд╢реНрд░реЗрдгреА рдХреЗ рдЖрдзрд╛рд░ рдкрд░)
3. рдЖрд╡реЗрджрди рдкреНрд░рдХреНрд░рд┐рдпрд╛ рд╕рдордЭрд╛рдирд╛
4. рдЖрд╡рд╢реНрдпрдХ рджрд╕реНрддрд╛рд╡реЗрдЬрд╝ рдмрддрд╛рдирд╛

ЁЯУЛ рдЙрдкрд▓рдмреНрдз рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛рдПрдВ:
{schemes_context}

ЁЯУМ рдорд╣рддреНрд╡рдкреВрд░реНрдг рдирд┐рдпрдо:
- рд╣рдореЗрд╢рд╛ рдХреЗрд╡рд▓ рд╣рд┐рдВрджреА рдореЗрдВ рдЬрд╡рд╛рдм рджреЗрдВ
- рд╕рдВрдХреНрд╖рд┐рдкреНрдд рдФрд░ рд╕реНрдкрд╖реНрдЯ рд░рд╣реЗрдВ (3-5 рд╡рд╛рдХреНрдп)
- рдЬрд╣рд╛рдВ рдЙрдЪрд┐рдд рд╣реЛ рдЗрдореЛрдЬреА рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ
- рдпрджрд┐ рдкрд╛рддреНрд░рддрд╛ рдЬрд╛рдВрдЪрдиреА рд╣реИ рддреЛ рдЙрдореНрд░ рдФрд░ рдЖрдп рдЬрд╝рд░реВрд░ рдкреВрдЫреЗрдВ
- рдпрджрд┐ рдЬрд╛рдирдХрд╛рд░реА рдЕрдзреВрд░реА рд╣реИ рддреЛ рд╡рд┐рдирдореНрд░рддрд╛ рд╕реЗ рдкреВрдЫреЗрдВ
- рдЭреВрдареА рдЬрд╛рдирдХрд╛рд░реА рди рджреЗрдВ - рдпрджрд┐ рдирд╣реАрдВ рдкрддрд╛ рддреЛ рдХрд╣реЗрдВ
- рд╣реЗрд▓реНрдкрд▓рд╛рдЗрди рдирдВрдмрд░ рдФрд░ рдЖрд╡реЗрджрди рд▓рд┐рдВрдХ рджреЗрдВ рдЬрдм рдЙрдкрдпреБрдХреНрдд рд╣реЛ

ЁЯЧгя╕П рдмрд╛рддрдЪреАрдд рдХрд╛ рддрд░реАрдХрд╛:
- рдорд┐рддреНрд░рд╡рдд рдФрд░ рд╕рдореНрдорд╛рдирдЬрдирдХ рд░рд╣реЗрдВ
- рд╕рд░рд▓ рднрд╛рд╖рд╛ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ
- рдЧрд░реАрдм рдФрд░ рдЧреНрд░рд╛рдореАрдг рд▓реЛрдЧреЛрдВ рдХреЛ рдзреНрдпрд╛рди рдореЗрдВ рд░рдЦреЗрдВ
- рдзреИрд░реНрдп рд╕реЗ рдЬрд╡рд╛рдм рджреЗрдВ"""

    def _build_user_context(self, user_data: Dict[str, Any], conversation_history: str) -> str:
        """Build context about the user"""
        context_parts = []
        
        if user_data:
            context_parts.append(f"рдЙрдкрдпреЛрдЧрдХрд░реНрддрд╛ рдХреА рдЬрд╛рдирдХрд╛рд░реА: {json.dumps(user_data, ensure_ascii=False)}")
        
        if conversation_history:
            context_parts.append(f"рдкрд┐рдЫрд▓реА рдмрд╛рддрдЪреАрдд:\n{conversation_history}")
        
        if context_parts:
            return "ЁЯУЭ рд╕рдВрджрд░реНрдн:\n" + "\n".join(context_parts)
        return ""
    
    def get_greeting(self) -> str:
        """Get initial greeting"""
        if not self.client:
            return self._default_greeting()
        
        try:
            prompt = """рдЖрдк "рд╕рд╣рд╛рдИ" рд╣реИрдВ - рдПрдХ рд╣рд┐рдВрджреА рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛ рд╕рд╣рд╛рдпрдХред
            
рдПрдХ рдорд┐рддреНрд░рд╡рдд рд╕реНрд╡рд╛рдЧрдд рд╕рдВрджреЗрд╢ рджреЗрдВ рдЬреЛ рдмрддрд╛рдП рдХрд┐ рдЖрдк рдХреНрдпрд╛-рдХреНрдпрд╛ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред
рд╕рдВрдХреНрд╖рд┐рдкреНрдд рд░рдЦреЗрдВ (4-5 рд╡рд╛рдХреНрдп)ред рдЗрдореЛрдЬреА рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВред"""

            from google.genai import types
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.8,
                    max_output_tokens=300
                )
            )
            
            return response.text.strip()
            
        except Exception as e:
            print(f"Greeting error: {e}")
            return self._default_greeting()
    
    def _default_greeting(self) -> str:
        """Default greeting when AI is unavailable"""
        return """рдирдорд╕реНрддреЗ! ЁЯЩП рдореИрдВ рд╕рд╣рд╛рдИ рд╣реВрдВ, рдЖрдкрдХрд╛ рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛ рд╕рд╣рд╛рдпрдХред

рдореИрдВ рдЖрдкрдХреЛ рдЗрди рдЪреАрдЬрд╝реЛрдВ рдореЗрдВ рдорджрдж рдХрд░ рд╕рдХрддрд╛ рд╣реВрдВ:
тАв рд╕рд░рдХрд╛рд░реА рдпреЛрдЬрдирд╛рдУрдВ рдХреА рдЬрд╛рдирдХрд╛рд░реА
тАв рдкрд╛рддреНрд░рддрд╛ рдЬрд╛рдВрдЪ
тАв рдЖрд╡реЗрджрди рдкреНрд░рдХреНрд░рд┐рдпрд╛

рдмрддрд╛рдЗрдП, рдЖрдкрдХреЛ рдХрд┐рд╕ рдпреЛрдЬрдирд╛ рдХреЗ рдмрд╛рд░реЗ рдореЗрдВ рдЬрд╛рдирдирд╛ рд╣реИ?"""

    def _fallback_response(self, user_input: str) -> str:
        """Fallback response when AI is unavailable"""
        return """рдХреНрд╖рдорд╛ рдХрд░реЗрдВ, рдореИрдВ рдЕрднреА рдЖрдкрдХреА рдорджрдж рдХрд░рдиреЗ рдореЗрдВ рдЕрд╕рдорд░реНрде рд╣реВрдВред

рдХреГрдкрдпрд╛ рдереЛрдбрд╝реА рджреЗрд░ рдмрд╛рдж рдлрд┐рд░ рд╕реЗ рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВ рдпрд╛ рд╣реЗрд▓реНрдкрд▓рд╛рдЗрди рдкрд░ рдХреЙрд▓ рдХрд░реЗрдВ: 1800-111-555"""
